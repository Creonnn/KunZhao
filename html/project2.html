<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" href="..\css\style.css">

        <!-- HighlightJS -->
        <link rel="stylesheet" href="..\packages\highlightjs\styles\codepen-embed.min.css"/>
        <script src="..\packages\highlightjs\highlight.min.js"></script>
        <script>hljs.highlightAll();</script>

        <!-- Renders LaTeX -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <title>Profile</title>
    </head>
    <body>
        <div class="mainNav">
            <a href="..\index.html">Home</a>
            <a href=".\projects.html">Projects</a>
            <a href=".\experience.html">Work Experience</a>
            <a href=".\education.html">Education</a>
            <a href=".\about.html">About</a>
        </div>
        <div class="tableOfContentsScroll">
            <ul>
                <li><a href="#intro"><big><b><p>Intro</p></b></big></a></li>
                <li><a href="#tools"><big><b><p>Tools</p></b></big></a></li>
                <li><a href="#theMathForForwardPropagation"><big><b><p>The Math for Forward Propagation</p></b></big></a></li>
                <ul>
                    <li><a href="#firstHiddenLayer"><big><b><p>First Hidden Layer</p></b></big></a></li>
                    <li><a href="#secondHiddenLayer"><big><b><p>Second Hidden Layer</p></b></big></a></li>
                    <li><a href="#output"><big><b><p>Output</p></b></big></a></li>
                </ul>
                <li><a href="#theMathForBackwardsPropagation"><big><b><p>The Math for Backwards Propagation</p></b></big></a></li>
                <ul>
                    <li><a href="#derivingTheErrorFunctionOfTheOutputLayer"><big><b><p>Deriving the error function of the output layer</p></b></big></a></li>
                    <li><a href="#derivingTheErrorFunctionOfEveryOtherLayer"><big><b><p>Deriving the error function of every other layer</p></b></big></a></li>
                    <li><a href="#derivingTheErrorOfTheWeights"><big><b><p>Deriving the error of the weights</p></b></big></a></li>
                    <li><a href="#derivingTheErrorOfTheBiases"><big><b><p>Deriving the error of the biases</p></b></big></a></li>
                    <li><a href="#derivingTheErrorOfTheWeightsOfTheLastLayer"><big><b><p>Deriving the error of the weights of the last layer</p></b></big></a></li>
                    <li><a href="#derivingTheErrorOfTheBiasesOfTheLastLayer"><big><b><p>Deriving the error of the biases of the last layer</p></b></big></a></li>
                    <li><a href="#equationsToUpdateWeightsAndBiases"><big><b><p>Equations to update weights and biases</p></b></big></a></li>
                    <li><a href="#equationsToUpdateExistingWeightsAndBiases"><big><b><p>Equations to update existing weights and biases</p></b></big></a></li>
                </ul>
                <li><a href="#codeImplementation"><big><b><p>Code Implementation</p></b></big></a></li>
                <ul>
                    <li><a href="#prerequisite"><big><b><p>Prerequisite</p></b></big></a></li>
                    <li><a href="#forwardPropagationFunctions"><big><b><p>Forward Propagation Functions</p></b></big></a></li>
                    <li><a href="#backwardsPropagationFunctions"><big><b><p>Backwards Propagation Functions</p></b></big></a></li>
                    <li><a href="#theLearningAlgorithm"><big><b><p>The Learning Algorithm</p></b></big></a></li>
                </ul>
                <br>
                <li><a href="#"><big><b><p>Top &#8613</p></b></big></a></li>
            </ul>
        </div>
        <div class="post">
            <h1>An intro to Neural Networks | MNIST Digit Classification</h1>
            <small>Published on: 8/21/2023</small>
            <hr color="black">


            <h2 id="intro"><strong>Intro</strong></h2>

            <p>I will be constructing a neural network without using any machine learning modules (Tensor Flow, PyTorch, etc). This project will focus on the mathematics behind the layers and optimization. This neural network will attempt to classify hand-written digits. The data set I used can be found <a href="https://www.kaggle.com/competitions/digit-recognizer/data" target="_blank">here</a>. The notebook with all the code can be found <a href="https://colab.research.google.com/drive/19r3ZRTi1M3QkW_emI75yz7KCy3hWKKD6?usp=sharing">here</a>.</p>

            <h2 id="tools"><strong>Tools</strong></h2>

            <ul>
                <li>Python</li>
                <li>Google Colab</li>
                <li>pandas</li>
                <li>numpy</li>
            </ul>

            <hr />
            <h2 id="theMathForForwardPropagation"><strong>The Math for Forward Propagation</strong></h2>

            <p>The math that makes everything work was probably the hardest aspect of this project -- likely because I haven&#39;t done pure calculus nor algebra in a while. However, it was well worth the time spent to understand the nuances of the math. There will be a ton of variables to keep track of indices and I will try my best to keep track of them so you don&#39;t forget what they are mid-way.</p>

            <p>We begin by constructing the structure of the neural network -- the forward propagation.</p>

            <p>Each image is \(28\times28=784\) pixels, so let \(x^{(i)}\) be a vector of length \(784\) which represents the pixels of the \(i^{th}\) image.</p>

            <h5><em>Note: each pixel is a value between \([0,1]\) where \(0\) is white and \(1\) is black.</em></h5>

            <p>Let \(m=\) number of training images.</p>

            <p>Now we can define all our training images as the matrix \(X=\begin{bmatrix}x^{(1)}&amp;x^{(2)}&amp;...&amp;x^{(m)}\end{bmatrix}\) where each \(x^{i}\) represents an image from our training set.</p>

            <p>We will have ten classes: \([0,1,2,3,4,5,6,7,8,9]\), ie. all the possible digits.</p>

            <p>We will use 3 layers: The input layer (784 neurons), two hidden layers (10 neurons each), and the output layer.</p>

            <p>The input layer takes one of the \(x^{i}\) (an image) as the input. Each of the 784 neurons takes a pixel from that image.&nbsp;</p>

            <p>The hidden layers will apply a linear transformation and an activation function on the neurons from the preceding layer.</p>

            <p>Finally, the output layer will attempt to classify the image based on the output from the previous layer.</p>

            <hr />
            <h3 id="firstHiddenLayer"><strong>First hidden layer</strong></h3>

            <p>We define the following:</p>

            <p>Let \(A^{[0]}_{784\times m} = X_{784\times m}\) be the input layer.&nbsp;</p>

            <p>Let \(Z^{[1]}_{10\times m}=w^{[1]}_{10\times 784}A^{[0]}_{784\times m}+b^{[1]}_{10\times1}\)</p>

            <p>where</p>

            <ul>
                <li>\(Z^{[1]}_{10\times m}\) is the unactivated neurons of the first hidden layer</li>
                <li>\(w^{[1]}_{10\times 784}\) is the weights matrix of the first hidden layer</li>
                <li>\(b^{[1]}_{10\times1}\) is the biases matrix of the first hidden layer</li>
            </ul>

            <p>We will be using the Rectified Linear Unit (ReLU) function as the activation function for the first hidden layer. It is defined as follows:</p>

            <p>\(ReLU(x)=\begin{cases}x&amp;x&gt;0\\0&amp;x\leq 0\end{cases}\)</p>

            <p>Furthermore,</p>

            <p>Let \(A^{[1]}=g(Z^{[1]})=ReLU(Z^{[1]})\)&nbsp;</p>

            <p>where</p>

            <ul>
                <li>\(A^{[1]}\) is the activated neurons of the first hidden layer</li>
                <li>\(g\) is the ReLU function</li>
            </ul>

            <hr />
            <h3 id="secondHiddenLayer"><strong>Second hidden layer</strong></h3>

            <p>Similar to the first hidden layer, we define the following:</p>

            <p>Let&nbsp;\(Z^{[2]}_{10\times m}=w^{[2]}_{10\times 10}A^{[1]}_{10\times m}+b^{[2]}_{10\times1}\)</p>

            <p>where</p>

            <ul>
                <li>\(Z^{[2]}_{10\times m}\) is the unactivated neurons of the&nbsp;second&nbsp;hidden layer</li>
                <li>\(w^{[2]}_{10\times 10}\) is the weights matrix of the second hidden layer</li>
                <li>\(A^{[1]}\) is the activated neurons from&nbsp;the first hidden layer</li>
                <li>\(b^{[2]}_{10\times1}\) is the biases matrix of the second&nbsp;hidden layer</li>
            </ul>

            <p>We will be using the Softmax function as the activation function for the second hidden layer. It is defined as follows:</p>

            <p>$$softmax(X)=\frac{e^{x_i}}{\sum_{j=1}^{k}e^{x_j}}$$</p>

            <p>where</p>

            <ul>
                <li>\(X\) is a vector of length \(k\)</li>
                <li>\(x_{i}\) is the \(i^{th}\) element of the vector \(X\)</li>
            </ul>

            <p>Thus, let&#39;s define:</p>

            <p>Let \(A^{[2]}=\)\(softmax(Z^{[2]})\)&nbsp;</p>

            <p>where</p>

            <ul>
                <li>&nbsp;\(A^{[2]}\) is the activated neurons of the second hidden layer&nbsp;</li>
                <li>\(Z^{[2]}\) is the unactivated neurons of the second hidden layer</li>
            </ul>

            <hr />
            <h3 id="output"><strong>Output</strong></h3>

            <p>The outpur layer essentially make a prediction based on the output of the previous layer (a vector of probabilities). So, the output will be the maximum value from that vector (ie. the highist probability) and classify the image based on the index of that maximum value.</p>

            <p>For example, suppose we have the following:</p>

            <p>\([0.1,0.1,0.1,0.3,0.4,0,0,0,0,0]\) -- the probabilities vector from second hidden layer.</p>

            <p>Then our output will return \(4\) because the maximum of that vector is \(0.4\) located at index \(4\).</p>

            <hr />
            <h2 id="theMathForBackwardsPropagation"><strong>The Math for Backwards Propagation</strong></h2>

            <p>When we begin the forward propagation process, the weight and biases are chosen at random within a given interval. Our job now after each epoch is to optimize each weights and biases such that we minimize the error. Since this model contains multiple classes (10 to be exact), we will use Categorical Cross Entropy as our cost function.</p>

            <p>Categorical Cross Entropy is defined as follows:</p>

            <p>$$L_i=-\sum_j y_{ij}ln(\hat{y}_{ij})$$</p>

            <p>where</p>

            <ul>
                <li>\(L_i=\)cost value of sample \(i\)</li>
                <li>\(y_{ij}=\)\(j^{th}\) element of \(y\) of sample \(i\)</li>
                <li>\(y=\)target values, one-hot encoded</li>
                <li>\(\hat{y}=\)predicted values</li>
            </ul>

            <p>Due to the one-hot encoding of \(y\) and the fact that there can only be one classification per image, our cost function simplifies to:</p>

            <p>\(L_i=-ln(\hat{y}_{ik})\) where \(k=\)target class.</p>

            <p>Note: Why Categorical Cross Entropy work well for this model?</p>

            <p>This is due to the behaviour of the \(log\) function.</p>

            <p>e.g. Let \(f(x)=-ln(x)\), where \(x\in (0,1]\)</p>

            <p>&nbsp; &nbsp; &nbsp; &nbsp;As \(x\rightarrow 0\), \(f(x)\rightarrow\infty\)</p>

            <p>&nbsp; &nbsp; &nbsp; &nbsp;In plain English, as our probability decreases for the prediction of our target class, our error increases.</p>

            <p>&nbsp; &nbsp; &nbsp; &nbsp;As&nbsp;\(x\rightarrow 1\), \(f(x)\rightarrow 0\)</p>

            <p>&nbsp; &nbsp; &nbsp; &nbsp;As our probability increases for the prediction of our target class, our error decreases.</p>

            <h3 id="derivingTheErrorFunctionOfTheOutputLayer"><strong>Deriving the error function of the output layer</strong></h3>
            <p>Let \(L=-\sum_i^{10} y_i ln(a_i)\) be the cost function.</p>

            <p>Where</p>

            <ul>
                <li>\(y_i\) is the \(i^{th}\) element of the one-hot encoded vector</li>
                <li>\(a_i\) is the \(i^{th}\) element of the output of our neural network. i.e. the activated neuron in the last layer</li>
            </ul>

            <p>Recall:</p>

            <p>$$a_i=\frac{e^{z_i}}{\sum_j^{10} e^{z_j}}$$</p>

            <p>Then, we will need to find the following:</p>

            <p>$$\frac{\partial{L}}{\partial{z_k}} = \sum_i^{10}&nbsp;\frac{\partial{L}}{\partial{a_i}}\frac{\partial{a_i}}{\partial{z_k}}$$</p>

            <p>$$=\sum_{i\neq k}^{10}&nbsp; (\frac{\partial{L}}{\partial{a_i}}\frac{\partial{a_i}}{\partial{z_k}})&nbsp;+ \frac{\partial{L}}{\partial{a_k}}\frac{\partial{a_k}}{\partial{z_k}}$$</p>

            <p>Solving for \(\frac{\partial{L}}{\partial{a_k}}\):</p>

            <p>$$\frac{\partial{L}}{\partial{a_k}}=\frac{\partial(-\sum_i^{10} y_i ln(a_i))}{\partial a_k}$$</p>

            <p>$$=\frac{\partial(-y_k ln(a_k))}{\partial a_k}$$</p>

            <p>$$=\frac{-y_k}{a_k}$$</p>

            <p style="text-align:center">\(\frac{\partial{L}}{\partial{a_k}}=\frac{-y_k}{a_k}\) ... Equation (1)</p>

            <p>Solving for \(\frac{\partial{a_k}}{\partial{z_k}}\):</p>

            <p>$$\frac{\partial{a_k}}{\partial{z_k}}=\frac{\partial(\frac{e^{z_k}}{\sum_i^{10} e^{z_i}})}{\partial z_k}$$</p>

            <p>$$=\frac{e^{z_k}(\sum_i^{10}e^{z_i})-e^{z_k}(e^{z_k})}{(\sum_i^{10}e^{z_i})^2}$$</p>

            <p>$$=\frac{e^{z_k}}{\sum_i^{10}e^{z_i}}(\frac{\sum_i^{10}e^{z_i}-e^{z_k}}{\sum_i^{10}e^{z_i}})$$</p>

            <p>$$=\frac{e^{z_k}}{\sum_i^{10}e^{z_i}}(1-\frac{e^{z_k}}{\sum_i^{10}e^{z_i}})$$</p>

            <p>$$=a_k(1-a_k)$$</p>

            <p style="text-align:center">\(\frac{\partial{a_k}}{\partial{z_k}}=a_k(1-a_k)\) ... Equation (2)</p>

            <p>For \(\frac{\partial{L}}{\partial{a_i}}\):</p>

            <p>$$\frac{\partial{L}}{\partial{a_i}}=\frac{\partial(-\sum_j^{10}y_j ln(a_j))}{\partial a_i}$$</p>

            <p>$$=\frac{-y_i}{a_i}$$</p>

            <p style="text-align:center">\(\frac{\partial{L}}{\partial{a_i}}=\frac{-y_i}{a_i}\) ... Equation (3)</p>

            <p>For \(\frac{\partial{a_i}}{\partial{z_k}}\):</p>

            <p>$$\frac{\partial{a_i}}{\partial{z_k}}=\frac{\partial(\frac{e^{z_i}}{\sum_j^{10}e^{z_j}})}{\partial z_k}$$</p>

            <p>$$=\frac{0\cdot(\sum_j^{10}e^{z_j})-e^{z_i}e^{z_k}}{(\sum_j^{10}e^{z_j})^2}$$</p>

            <p>$$=-(\frac{e^{z_i}}{\sum_j^{10}e^{z_j}})(\frac{e^{z_k}}{\sum_j^{10}e^{z_j}})$$</p>

            <p>$$ =-a_ia_k$$</p>

            <p style="text-align:center">\(\frac{\partial{a_i}}{\partial{z_k}}=-a_ia_k\) ... Equation (4)</p>

            <p>To summarize, we have the following:</p>

            <ul>
                <li>\(\frac{\partial{L}}{\partial{a_k}}=\frac{-y_k}{a_k}\) ... Equation (1)</li>
                <li>\(\frac{\partial{a_k}}{\partial{z_k}}=a_k(1-a_k)\) ... Equation (2)</li>
                <li>\(\frac{\partial{L}}{\partial{a_i}}=\frac{-y_i}{a_i}\) ... Equation (3)</li>
                <li>\(\frac{\partial{a_i}}{\partial{z_k}}=-a_ia_k\) ... Equation (4)</li>
            </ul>

            <p>Putting everything back into the original equation, we have:</p>

            <p>$$\frac{\partial{L}}{\partial{z_k}} =\sum_{i\neq k}^{10}&nbsp; (\frac{\partial{L}}{\partial{a_i}}\frac{\partial{a_i}}{\partial{z_k}})&nbsp;+ \frac{\partial{L}}{\partial{a_k}}\frac{\partial{a_k}}{\partial{z_k}}$$</p>

            <p>$$=\sum_{i\neq k}^{10}((\frac{-y_i}{a_i})(-a_ia_k))+(\frac{-y_k}{a_k})(a_k(1-a_k))$$</p>

            <p>$$=\sum_{i\neq k}^{10}(y_ia_k)+(-y_k(1-a_k))$$</p>

            <p>$$=\sum_{i\neq k}^{10}y_ia_k+y_ka_k-y_k$$</p>

            <p>$$=\sum_{i=1}^{10}y_ia_k-y_k$$</p>

            <p>$$=a_k\sum_{i=1}^{10}y_i-y_k$$</p>

            <p>Note: Since \(y\) is one-hot encoded, it must be that \(\sum_i^{10}y_i=1\).</p>

            <p>Hence we can further simplify to the following:</p>

            <p>$$=a_k\sum_{i=1}^{10}y_i-y_k$$</p>

            <p>$$=a_k\cdot 1 -y_k$$</p>

            <p>$$=a_k-y_k$$</p>

            <p>In vector form, the above is simply just \(\frac{\partial L}{\partial Z^{[L]}}=a^{[L]}-y\) where \([L]\) is the last layer.&nbsp;(Note: \([L]\) is put in the superscript and DOES NOT mean exponent/power)</p>

            <h3 id="derivingTheErrorFunctionOfEveryOtherLayer"><strong>Deriving the error function of every other layer</strong></h3>

            <p><strong>**Important** Recall the structure of our forward propagation:</strong></p>

            <p>$$z_k^{[l]} = z_k^{[l]}(a_1^{[l-1]}, a_2^{[l-2]},...,a_{n^{[l-1]}}^{[l-1]})$$</p>

            <p>$$=w_{k, 1}^{[l]}a_1^{[l-1]}+w_{k,2}^{[l]}a_2^{[l-1]}+...+w_{k,n^{[l-1]}}^{[l]}a_{n^{[l-1]}}^{[l-1]}+b_k^{[l]}$$&nbsp;</p>

            <p>Our goal is to derive \(\frac{\partial L}{\partial z^{[l-1]}}\) where \(l\) represents the \(l^{th}\) layer.</p>

            <p>Consider the following:</p>

            <p>$$\frac{\partial L}{\partial z_i^{[l-1]}}=\sum_{k=1}^{n^{[l-1]}}\frac{\partial L}{\partial a_k^{[l-1]}}\frac{\partial g^{[l-1]}}{\partial z_i^{[l-1]}}$$</p>

            <p>where:</p>

            <ul>
                <li>\(L\) is the loss function</li>
                <li>\(z\) is the unactivated neuron</li>
                <li>\(a\) is the activated neuron</li>
                <li>\(g\) is the activation function</li>
                <li>\(i\) is the index&nbsp;of the&nbsp;neuron of the layer that it&#39;s in</li>
                <li>\([l]\) is the layer index (Note: this is put in the superscript and DOES NOT mean exponent/power)</li>
                <li>\(n\) is the total number of neurons in layer \(l\)</li>
            </ul>

            <p>Note:</p>

            <p>$$\frac{\partial g^{[l-1]}}{\partial z_i^{[l-1]}}=\begin{cases}g^{[l-1](1)}(z_k^{[l-1]})&amp;,k=i\\ 0&amp; ,otherwise\end{cases}$$</p>

            <p>This is because \(g^{[l-1]}\) only takes one \(z\) as the input each time. Also, the superscript \((1)\) represents the first derivative.</p>

            <p>Thus, the summation collapses and simplifies to the following:</p>

            <p>$$\frac{\partial L}{\partial z_i^{[l-1]}}=\frac{\partial L}{\partial a_i^{[l-1]}}g^{[l-1](1)}(z_i^{l-1})$$</p>

            <p>We can generalize the above in terms of matrix operations as follows.</p>

            <p>\(\frac{\partial L}{\partial z^{[l-1]}}=\frac{\partial L}{\partial a^{[l-1]}}\cdot g^{[l-1](1)}(z^{[l-1]})\) ... Equation (5), with term-wise multiplication</p>

            <p>where</p>

            <ul>
                <li>\(\frac{\partial L}{\partial a^{[l-1]}}=\begin{bmatrix}\frac{\partial L}{\partial a_1^{[l-1]}}\\\frac{\partial L}{\partial a_2^{[l-1]}}\\...\\\frac{\partial L}{\partial a_{n^{[l-1]}}^{[l-1]}}\end{bmatrix}\)</li>
                <li>\(g^{[l-1](1)}(z^{[l-1]})=\begin{bmatrix}g^{[l-1](1)}(z_1^{[l-1]})\\g^{[l-1](1)}(z_2^{[l-1]})\\...\\g^{[l-1](1)}(z_{n^{[l-1]}}^{[l-1]})\end{bmatrix}\)</li>
            </ul>

            <p>So, we know what \(g\) is because it&#39;s predefined during forward propagation. We need to determine what \(\frac{\partial L}{\partial a_i^{[l-1]}}\) is.</p>

            <p>For \(\frac{\partial L}{\partial a_i^{[l-1]}}\):</p>

            <p>We have the following.</p>

            <p>$$\frac{\partial L}{\partial a_i^{[l-1]}}=\sum_{k=1}^{n^{[l]}}\frac{\partial L}{\partial z_k^{[l]}}\frac{\partial z_k^{[l]}}{\partial a_i^{[l-1]}}$$</p>

            <p>Note:</p>

            <p>$$\frac{\partial z_k^{[l]}}{\partial a_i^{[l-1]}}=w_{k, i}^{[l]}$$</p>

            <p>This is because every \(a_j^{[l-1]}\) are considered constants for \(j\neq i\).</p>

            <p>Thus we can simplify as follows:</p>

            <p>$$\frac{\partial L}{\partial a_i^{[l-1]}}=\sum_{k=1}^{n^{[l]}}\frac{\partial L}{\partial z_k^{[l]}}\frac{\partial z_k^{[l]}}{\partial a_i^{[l-1]}}=\sum_{k=1}^{n^{[l]}}\frac{\partial L}{\partial z_k^{[l]}}w_{k, i}^{[l]}$$</p>

            <p>We can generalize the above in terms of matrix operations as follows.</p>

            <p>\(\frac{\partial L}{\partial a^{[l-1]}}=w^{[l]}\cdot \frac{\partial L}{\partial z^{[l]}}\) ... Equation (6)</p>

            <p>where</p>

            <ul>
                <li>\(w^{[l]}=\begin{bmatrix}w_{1,1}^{[l]}&amp;w_{2,1}^{[l]}&amp;...&amp;w_{n^{[l]},1}^{[l]}\\ w_{1,2}^{[l]}&amp;w_{2,2}^{[l]}&amp;...&amp;w_{n^{[l]},2}^{[l]}\\...&amp;...&amp;...&amp;...\\w_{1,n^{[l-1]}}^{[l]}&amp;w_{2,n^{[l-1]}}^{[l]}&amp;...&amp;w_{n^{[l]},n^{[l-1]}}^{[l]}\end{bmatrix}\)</li>
                <li>\(\frac{\partial L}{\partial z^{[l]}}=\begin{bmatrix}\frac{\partial L}{\partial z_1^{[l]}}\\\frac{\partial L}{\partial z_2^{[l]}}\\...\\\frac{\partial L}{\partial z_{n^{[l]}}^{[l]}}\end{bmatrix}\)</li>
            </ul>

            <p>Substituting Equation (6) into Equation (5), we obtain the following:</p>

            <p>$$\frac{\partial L}{\partial z^{[l-1]}}=\frac{\partial L}{\partial a^{[l-1]}}\cdot g^{[l-1](1)}(z^{[l-1]})$$</p>

            <p>$$=w^{[l]}\frac{\partial L}{\partial z^{[l]}}g^{[l-1](1)}(z^{[l-1]})$$</p>

            <h3 id="derivingTheErrorOfTheWeights"><strong>Deriving the error of the weights</strong></h3>

            <p>Goal: Derive \(\frac{\partial L}{\partial w^{[l]}}\)</p>

            <p>Consider the following.</p>

            <p>$$\frac{\partial L}{\partial w_{i,j}^{[l]}}=\sum_{k=1}^{n^{[l]}}\frac{\partial L}{\partial z_k^{[l]}}\frac{\partial z_k^{[l]}}{\partial w_{i,j}^{[l]}}$$</p>

            <p>Note: \(\frac{\partial z_k^{[l]}}{\partial w_{i,j}^{[l]}}=\begin{cases}a_j^{[l-1]}&amp;,k=i\\0&amp;,otherwise\end{cases}\)</p>

            <p>Hence we obtain the following.</p>

            <p>$$\frac{\partial L}{\partial w_{i,j}^{[l]}}=\frac{\partial L}{\partial z_{i,j}^{[l]}}a_j^{[l-1]}$$</p>

            <p>$$\Rightarrow \frac{\partial L}{\partial w^{[l]}}=\frac{\partial L}{\partial z^{[l]}}(a^{[l-1]})^T$$</p>

            <h3 id="derivingTheErrorOfTheBiases"><strong>Deriving the error of the biases</strong></h3>

            <p>Goal: Derive \(\frac{\partial L}{\partial b^{[l]}}\)</p>

            <p>Consider the following.</p>

            <p>$$\frac{\partial L}{\partial b_i^{[l]}}=\sum_{k=1}^{n^{[l]}}\frac{\partial L}{\partial z_k^{[l]}}\frac{\partial z_k^{[l]}}{\partial b_i^{[l]}}$$</p>

            <p>Note: \(\frac{\partial z_k^{[l]}}{\partial b_i^{[l]}}=\begin{cases}1&amp;,k=i\\0&amp;,otherwise\end{cases}\)</p>

            <p>Thus we get the following.</p>

            <p>$$\frac{\partial L}{\partial b_i^{[l]}}=\frac{\partial L}{\partial z_i^{[l]}}$$</p>

            <p>$$\Rightarrow \frac{\partial L}{\partial b^{[l]}}=\frac{\partial L}{\partial z^{[l]}}$$</p>

            <h3 id="derivingTheErrorOfTheWeightsOfTheLastLayer"><strong>Deriving the error of the weights of the last layer</strong></h3>

            <p>Let \(l\) be the index of the last layer.</p>

            <p>Then,</p>

            <p>$$\frac{\partial L}{/\partial w^{[l]}}=\frac{\partial L}{\partial z^{[l]}}(a^{[l-1]})^T=(a^{[l]}-y)(a^{[l-1]})^T$$</p>

            <p>However, the above equation represents the total error of all the weights.&nbsp;</p>

            <p>Thus, let \(t\) be the number of training data points and when can obtain the average error as follows.</p>

            <p>$$Error_{w^{[l]}}=\frac{1}{t}(a^{[l]}-y)(a^{[l-1]})^T$$</p>

            <h3 id="derivingTheErrorOfTheBiasesOfTheLastLayer"><strong>Deriving the error of the biases of the last layer</strong></h3>

            <p>Let \(l\) be the index of the last layer and&nbsp;\(t\) be the number of training data points.</p>

            <p>Then,</p>

            <p>$$\frac{\partial L}{\partial b^{[l]}}=\frac{\partial L}{\partial z^{[l]}}=\sum_j^{t}(a^{[l]}-y)$$</p>

            <p>With similar reasoning, we need to divide by \(t\).</p>

            <p>$$Error_{b^{[l]}}=\frac{1}{t}\sum_j^{t}(a^{[l]}-y)$$</p>

            <h3 id="equationsToUpdateWeightsAndBiases"><strong>Equations to update weights and biases</strong></h3>

            <p>Now we can definine equations to update existing weights and biases for each layer depening on the respective errors.</p>

            <p>Recall: we have 2 hidden layers and each layer has the following characteristics.</p>

            <p style="margin-left:40px">&nbsp;Layer 1:</p>

            <ul>
                <li>\(Z^{[1]}_{10\times m}=w^{[1]}_{10\times 784}A^{[0]}_{784\times m}+b^{[1]}_{10\times1}\)</li>
                <li>\(A^{[1]}=g(Z^{[1]})=ReLU(Z^{[1]})\)&nbsp;</li>
            </ul>

            <p style="margin-left:40px">Layer 2:</p>

            <ul>
                <li>\(Z^{[2]}_{10\times m}=w^{[2]}_{10\times 10}A^{[1]}_{10\times m}+b^{[2]}_{10\times1}\)</li>
                <li>\(A^{[2]}=\)\(softmax(Z^{[2]})\)</li>
            </ul>

            <p>The equation of the errors are defined as follows.</p>

            <ul>
                <li>\(Error_{Z^{[2]}}=A^{[2]}-y\)</li>
                <li>\(Error_{W^{[2]}}=\frac{1}{m}Error_{Z^[2]}A^{[1]T}\)</li>
                <li>\(Error_{b^{[2]}}=\frac{1}{m}\sum Error_{Z^{[2]}}\)</li>
                <li>\(Error_{Z^{[1]}}=W^{[2]T}Error_{Z^{[2]}}ReLU^{(1)}(Z^{[1]})\)</li>
                <li>\(Error_{W^{[1]}}=\frac{1}{m}Error_{Z^{[1]}}X^T\)</li>
                <li>\(Error_{b^{[1]}}=\frac{1}{m}\sum Error_{Z^{[1]}}\)</li>
            </ul>

            <h3 id="equationsToUpdateExistingWeightsAndBiases"><strong>Equations to update existing weights and biases</strong></h3>

            <p>Let \(\alpha\) be the learning rate.</p>

            <ul>
                <li>\(New_{W^{[1]}}=W^{[1]}-\alpha Error_{W^{[1]}}\)</li>
                <li>\(New_{b^{[1]}}=b^{[1]}-\alpha Error_{b^{[1]}}\)</li>
                <li>\(New_{W^{[2]}}=W^{[2]}-\alpha Error_{W^{[2]}}\)</li>
                <li>\(New_{b^{[2]}}=b^{[2]}-\alpha Error_{b^{[2]}}\)</li>
            </ul>

            <hr />
            <h2 id="codeImplementation"><strong>Code implementation</strong></h2>

            <h3 id="prerequisite"><strong>Prerequisite</strong></h3>

            <p>Import packages and data.</p>

            <p><pre><code>&gt;&gt;&gt;import numpy as np

&gt;&gt;&gt;import pandas as pd

&gt;&gt;&gt;data = pd.read_csv(&quot;train.csv&quot;)</code></pre></p>

            <p>Shuffle and split the data into a validation set and a training set.</p>

            <p><pre><code>&gt;&gt;&gt;data_dev = data[0:1000].T

&gt;&gt;&gt;Y_dev = data_dev[0]

&gt;&gt;&gt;X_dev = data_dev[1:n]

&gt;&gt;&gt;X_dev = X_dev / 255.

&gt;&gt;&gt;data_train = data[1000:m].T

&gt;&gt;&gt;Y_train = data_train[0]

&gt;&gt;&gt;X_train = data_train[1:n]

&gt;&gt;&gt;X_train = X_train / 255.</code></pre></p>

            <h3 id="forwardPropagationFunctions"><strong>Forward Propagation Functions</strong></h3>

            <p>Initialize weights and biases.</p>

            <p><pre><code>&gt;&gt;&gt;def init_params():

&nbsp; &nbsp; &nbsp;W1 = np.random.rand(10, 784) - 0.5

&nbsp; &nbsp; &nbsp;b1 = np.random.rand(10, 1) - 0.5

&nbsp; &nbsp; &nbsp;W2 = np.random.rand(10, 10) - 0.5

 &nbsp; &nbsp; &nbsp;b2 = np.random.rand(10, 1) - 0.5

&nbsp; &nbsp; &nbsp;return W1, b1, W2, b2</code></pre></p>

            <p>Defining the ReLU function.</p>

            <p><pre><code>&gt;&gt;&gt;def ReLU(Z):

&nbsp; &nbsp; &nbsp;return np.maximum(Z, 0) # Performs element-wise comparison with matrix Z to the number 0

>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# ie, if Z_{jk} &lt;= 0, returns 0. If Z_{jk} &gt; 0, returns Z_{jk}</code></pre></p>

            <p>Defining the softmax function.</p>

            <p><pre><code>&gt;&gt;&gt;def softmax(Z):

&nbsp; &nbsp; &nbsp;A = np.exp(Z) / sum(np.exp(Z)) # Definition of the softmax function. We use Python&#39;s built-in sum because it&#39;ll add the values&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # column-wise

&nbsp; &nbsp; &nbsp;return A</code></pre></p>

            <p>Defining one-hot encoding.</p>

            <p><pre><code>&gt;&gt;&gt;def one_hot(Y):

&nbsp; &nbsp; &nbsp;one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # Y.size returns the number of elements in Y, aka m

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Y.max() = 9, thus Y.max + 1 = 10. Recall: Y here is our labels array

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Hence, (Y.size, Y.max + 1) describes the dimension of one_hot_Y

&nbsp;

&nbsp; &nbsp; &nbsp;one_hot_Y[np.arange(Y.size), Y] = 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Accessing each row by index [j, k] = [np.arange(Y.size), Y] and set that value to 1

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# np.arange(Y.size) = [0, 1, 2, ... , m]

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Y = labels array

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Hence, each row represents a label but one-hot-encoded

&nbsp;

&nbsp; &nbsp; &nbsp;one_hot_Y = one_hot_Y.T&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Transposing

&nbsp; &nbsp; &nbsp;return one_hot_Y</code></pre></p>

            <p>Defining forward propagation function.</p>

            <p><pre><code>&gt;&gt;&gt;def forward_prop(W1, b1, W2, b2, X):

&nbsp; &nbsp; &nbsp;Z1 = W1.dot(X) + b1 &nbsp; # Apply linear transformation on X

&nbsp; &nbsp; &nbsp;A1 = ReLU(Z1) &nbsp; &nbsp; &nbsp; &nbsp; # Activating Z1 using activation function ReLU

&nbsp; &nbsp; &nbsp;Z2 = W2.dot(A1) + b2 &nbsp;# Apply linear transformation on A1

&nbsp; &nbsp; &nbsp;A2 = softmax(Z2) &nbsp; &nbsp; &nbsp;# Activating Z2 using actication function softmax

&nbsp; &nbsp; &nbsp;return Z1, A1, Z2, A2</code></pre></p>

            <p>Defining the derivative of the ReLU function.</p>

            <p><pre><code>&gt;&gt;&gt;def deriv_ReLU(Z):

&nbsp; &nbsp; &nbsp;return Z &gt; 0 # Recall True = 1, False = 0</code></pre></p>

            <h3 id="backwardsPropagationFunctions"><strong>Backwards Propagation Functions</strong></h3>

            <p>Defining the function to calculate errors.</p>

            <p><pre><code>&gt;&gt;&gt;def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):

&nbsp; &nbsp; &nbsp;m = Y.size

&nbsp; &nbsp; &nbsp;one_hot_Y = one_hot(Y) # Perform one-hot-encoding on Y

&nbsp; &nbsp; &nbsp;error_Z2 = A2 - one_hot_Y &nbsp; # Calculate error

&nbsp; &nbsp; &nbsp;error_W2 = 1 / m * error_Z2.dot(A1.T)

&nbsp; &nbsp; &nbsp;error_b2 = 1 / m * np.sum(error_Z2)

&nbsp; &nbsp; &nbsp;error_Z1 = W2.T.dot(error_Z2) * deriv_ReLU(Z1)

&nbsp; &nbsp; &nbsp;error_W1 = 1 / m * error_Z1.dot(X.T)

&nbsp; &nbsp; &nbsp;error_b1 = 1 / m * np.sum(error_Z1)

&nbsp; &nbsp; &nbsp;return error_W1, error_b1, error_W2, error_b2</code></pre></p>

            <p>Defining the function to update weights and biases.</p>

            <p><pre><code>&gt;&gt;&gt;def update_params(W1, b1, W2, b2, error_W1, error_b1, error_W2, error_b2, alpha):

&nbsp; &nbsp; &nbsp;new_W1 = W1 - alpha * error_W1

&nbsp; &nbsp; &nbsp;new_b1 = b1 - alpha * error_b1

&nbsp; &nbsp; &nbsp;new_W2 = W2 - alpha * error_W2

&nbsp; &nbsp; &nbsp;new_b2 = b2 - alpha * error_b2

&nbsp; &nbsp; &nbsp;return new_W1, new_b1, new_W2, new_b2</code></pre></p>

            <h3 id="theLearningAlgorithm"><strong>The Learning Algorithm</strong></h3>

            <p>Function for extracting the prediction from the output of our neural network.</p>

            <p><pre><code>&gt;&gt;&gt;def get_predictions(A2):

&nbsp; &nbsp; &nbsp;return np.argmax(A2, 0) # Returns index of greatest value per column</code></pre></p>

            <p>Function for calculating the accuracy of the prediction.</p>

            <p><pre><code>&gt;&gt;&gt;def get_accuracy(predictions, Y):

&nbsp; &nbsp; &nbsp;print(predictions, Y)

&nbsp; &nbsp; &nbsp;return np.sum(predictions == Y) / Y.size</code></pre></p>

            <p>Function for gradient descent (continuous iteration through the training set and optimize the weights and biases each time).</p>

            <p><pre><code>&gt;&gt;&gt;def gradient_descent(X, Y, iterations, alpha):

&nbsp; &nbsp; &nbsp;W1, b1, W2, b2 = init_params()

&nbsp; &nbsp; &nbsp;for i in range(iterations):

&nbsp; &nbsp; &nbsp; &nbsp;Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)

&nbsp; &nbsp; &nbsp; &nbsp;error_W1, error_b1, error_W2, error_b2 = back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)

&nbsp; &nbsp; &nbsp; &nbsp;W1, b1, W2, b2 = update_params(W1, b1, W2, b2, error_W1, error_b1, error_W2, error_b2, alpha)

&nbsp; &nbsp; &nbsp; &nbsp;if (i % 10) == 0:

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(&quot;Iterations&quot;, i)

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(&quot;Accuracy&quot;, get_accuracy(get_predictions(A2), Y))

&nbsp; &nbsp; &nbsp;return W1, b1, W2, b2</code></pre></p>

            <p>Now we can run the following to train our neural network.</p>

            <p><pre><code>&gt;&gt;&gt;W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 500, 0.10)</code></pre></p>

        </div>
    </body>
</html>